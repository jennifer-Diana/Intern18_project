{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15aca6a5",
   "metadata": {},
   "source": [
    "##Install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "057cdc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\my pc\\anaconda3\\lib\\site-packages (from selenium) (1.26.4)\n",
      "Installing collected packages: selenium\n",
      "Successfully installed selenium-3.141.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "929947df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9840170d",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "627bd734",
   "metadata": {},
   "outputs": [],
   "source": [
    "##opening chrome window\n",
    "driver=webdriver.Chrome(\"C:\\\\Users\\\\my pc\\\\Downloads\\\\chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "b7d4ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##opening the webpage\n",
    "url=\"https://www.naukri.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ba842650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"1197d4ca3e3917c71eda2d879a8f51bf\", element=\"95d21421-6fee-4346-9697-a81beed62cce\")>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##getting web element of search bar\n",
    "designation=driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "96044173",
   "metadata": {},
   "outputs": [],
   "source": [
    "##searching for data analyst\n",
    "designation.send_keys(\"Data Analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "33215faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"1197d4ca3e3917c71eda2d879a8f51bf\", element=\"98946404-ef72-4933-939f-f51d4120ce74\")>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##getting web element of location\n",
    "loc=driver.find_element_by_id(\"qsb-location-sugg\")\n",
    "loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "873542b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## selecting banglore as location\n",
    "loc.send_keys(\"Banglore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "626d2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "##clicking search button\n",
    "button1=driver.find_element_by_class_name(\"btn\")\n",
    "button1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "eab07eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "company=[]\n",
    "exp=[]\n",
    "locat=[]\n",
    "##scraping web elements job title,name,location,experience\n",
    "job=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "company_name=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "exp_tag=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span[1]\")\n",
    "jobloc=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span[1]\")\n",
    "##extracting text from webelements\n",
    "for i in job: \n",
    "        job_title.append(i.text.replace(\"\\n\",' ')) \n",
    "for i in company_name: \n",
    "        company.append(i.text.replace(\"\\n\",' ')) \n",
    "for i in exp_tag: \n",
    "    exp.append(i.text.replace(\"\\n\",' '))\n",
    "\n",
    "for i in jobloc: \n",
    "    locat.append(i.text.replace(\"\\n\",' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "929d8b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_title</th>\n",
       "      <th>company_Name</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business Data Analyst</td>\n",
       "      <td>Trigent Software</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business Data Analyst</td>\n",
       "      <td>Trigent Software</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst/Business Analyst-Gurgaon/Bangalor...</td>\n",
       "      <td>India Medtronic Pvt. Ltd,.</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru, Mumbai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>WEIWO Communication Pvt. Ltd.</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru(Ulsoor)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>GCC SERVICES INDIA PRIVATE LIMITED</td>\n",
       "      <td>5-9 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Systems and Data Analyst, Safety and Provisioning</td>\n",
       "      <td>AECOM India Private Limited</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Allegis Services India Pvt. Ltd.</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Allegis Services India Pvt. Ltd.</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Enzen Global Solutions Pvt. Ltd</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Analyst - SQL/Tableau/Redshift</td>\n",
       "      <td>Pronto Consulting Services</td>\n",
       "      <td>6-10 Yrs</td>\n",
       "      <td>Noida, Mumbai, Indore, Hyderabad/Secunderabad,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_title  \\\n",
       "0                              Business Data Analyst   \n",
       "1                              Business Data Analyst   \n",
       "2  Data Analyst/Business Analyst-Gurgaon/Bangalor...   \n",
       "3                                       Data Analyst   \n",
       "4                                       Data Analyst   \n",
       "5  Systems and Data Analyst, Safety and Provisioning   \n",
       "6                                       Data Analyst   \n",
       "7                                       Data Analyst   \n",
       "8                                       Data Analyst   \n",
       "9         Senior Data Analyst - SQL/Tableau/Redshift   \n",
       "\n",
       "                         company_Name Experience  \\\n",
       "0                    Trigent Software    3-5 Yrs   \n",
       "1                    Trigent Software    3-5 Yrs   \n",
       "2          India Medtronic Pvt. Ltd,.    1-4 Yrs   \n",
       "3       WEIWO Communication Pvt. Ltd.    4-8 Yrs   \n",
       "4  GCC SERVICES INDIA PRIVATE LIMITED    5-9 Yrs   \n",
       "5         AECOM India Private Limited    3-5 Yrs   \n",
       "6    Allegis Services India Pvt. Ltd.    2-5 Yrs   \n",
       "7    Allegis Services India Pvt. Ltd.    3-5 Yrs   \n",
       "8     Enzen Global Solutions Pvt. Ltd    6-8 Yrs   \n",
       "9          Pronto Consulting Services   6-10 Yrs   \n",
       "\n",
       "                                            Location  \n",
       "0                                Bangalore/Bengaluru  \n",
       "1                                Bangalore/Bengaluru  \n",
       "2  Gurgaon/Gurugram, Bangalore/Bengaluru, Mumbai ...  \n",
       "3                        Bangalore/Bengaluru(Ulsoor)  \n",
       "4                                Bangalore/Bengaluru  \n",
       "5                                Bangalore/Bengaluru  \n",
       "6                                Bangalore/Bengaluru  \n",
       "7                                Bangalore/Bengaluru  \n",
       "8                                Bangalore/Bengaluru  \n",
       "9  Noida, Mumbai, Indore, Hyderabad/Secunderabad,...  "
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##create a dataframe of the scraped data.\n",
    "data_A=pd.DataFrame({})\n",
    "data_A['Job_title']=job_title\n",
    "data_A['company_Name']=company\n",
    "data_A['Experience']=exp\n",
    "data_A['Location']=locat\n",
    "data_A[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb0209",
   "metadata": {},
   "source": [
    "Q2.Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "00cd55c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##opening the webpage\n",
    "url1=\"https://www.naukri.com/\"\n",
    "driver.get(url1)\n",
    "##searching for data scientist\n",
    "designation=driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "designation.send_keys(\"Data Scientist\")\n",
    "## selecting banglore as location\n",
    "loc=driver.find_element_by_id(\"qsb-location-sugg\")\n",
    "loc.send_keys(\"Banglore\")\n",
    "##clicking search button\n",
    "button1=driver.find_element_by_class_name(\"btn\")\n",
    "button1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "26893762",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "company=[]\n",
    "exp=[]\n",
    "locat=[]\n",
    "##scraping web elements job title,name,location,experience\n",
    "job=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "company_name=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "exp_tag=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span[1]\")\n",
    "jobloc=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span[1]\")\n",
    "##extracting text from webelements\n",
    "for i in job: \n",
    "        job_title.append(i.text.replace(\"\\n\",' ')) \n",
    "for i in company_name: \n",
    "        company.append(i.text.replace(\"\\n\",' ')) \n",
    "for i in exp_tag: \n",
    "    exp.append(i.text.replace(\"\\n\",' '))\n",
    "\n",
    "for i in jobloc: \n",
    "    locat.append(i.text.replace(\"\\n\",' '))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3791d6",
   "metadata": {},
   "source": [
    "2. scrape full job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "c1df99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "link=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "##getting webelement of title tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "25e20663",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=[]\n",
    "for i in link:\n",
    "    url.append(i.get_attribute(\"href\"))\n",
    "##getting all url's attribure href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "5d7d6ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_title</th>\n",
       "      <th>company_Name</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Location</th>\n",
       "      <th>Job desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Philips India Limited</td>\n",
       "      <td>8-10 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Be a part of the Enterprise IT , Information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist: Advanced Analytics</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>As a Data Scientist at IBM, you will help tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>SYMBIOSIS International W.L.L</td>\n",
       "      <td>10-15 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Required Candidate profile We are looking for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DATA SCIENTIST</td>\n",
       "      <td>McAfee Software (India) Pvt. Ltd</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Data Scientist Primary Location India, Bangalo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Datamatics Global Services Ltd</td>\n",
       "      <td>8-13 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Roles and Responsibilities  Details in lined f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Software Developer - Data Scientist / NLP / Ma...</td>\n",
       "      <td>Cunesoft India Private Limited</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Dear Candidates We are looking for a competent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data scientist</td>\n",
       "      <td>Uber</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>About the role We are looking for skilled moti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>We are looking for an experienced Data Scienti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Publicis Groupe</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Work location: Bengaluru (Currently, Remote) S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bidgely Technologies Private Limited</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Bidgely is looking for extraordinary and dynam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_title  \\\n",
       "0                              Senior Data Scientist   \n",
       "1                 Data Scientist: Advanced Analytics   \n",
       "2                              Senior Data Scientist   \n",
       "3                                     DATA SCIENTIST   \n",
       "4                              Senior Data Scientist   \n",
       "5  Software Developer - Data Scientist / NLP / Ma...   \n",
       "6                                     Data scientist   \n",
       "7                              Senior Data Scientist   \n",
       "8                              Senior Data Scientist   \n",
       "9                              Senior Data Scientist   \n",
       "\n",
       "                           company_Name Experience             Location  \\\n",
       "0                 Philips India Limited   8-10 Yrs  Bangalore/Bengaluru   \n",
       "1                IBM India Pvt. Limited   5-10 Yrs  Bangalore/Bengaluru   \n",
       "2         SYMBIOSIS International W.L.L  10-15 Yrs  Bangalore/Bengaluru   \n",
       "3      McAfee Software (India) Pvt. Ltd    4-8 Yrs  Bangalore/Bengaluru   \n",
       "4        Datamatics Global Services Ltd   8-13 Yrs  Bangalore/Bengaluru   \n",
       "5        Cunesoft India Private Limited    3-8 Yrs  Bangalore/Bengaluru   \n",
       "6                                  Uber    6-8 Yrs  Bangalore/Bengaluru   \n",
       "7                IBM India Pvt. Limited   5-10 Yrs  Bangalore/Bengaluru   \n",
       "8                       Publicis Groupe    2-5 Yrs  Bangalore/Bengaluru   \n",
       "9  Bidgely Technologies Private Limited    4-6 Yrs  Bangalore/Bengaluru   \n",
       "\n",
       "                                            Job desc  \n",
       "0    Be a part of the Enterprise IT , Information...  \n",
       "1  As a Data Scientist at IBM, you will help tran...  \n",
       "2  Required Candidate profile We are looking for ...  \n",
       "3  Data Scientist Primary Location India, Bangalo...  \n",
       "4  Roles and Responsibilities  Details in lined f...  \n",
       "5  Dear Candidates We are looking for a competent...  \n",
       "6  About the role We are looking for skilled moti...  \n",
       "7  We are looking for an experienced Data Scienti...  \n",
       "8  Work location: Bengaluru (Currently, Remote) S...  \n",
       "9  Bidgely is looking for extraordinary and dynam...  "
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_desc=[]\n",
    "for i in url:\n",
    "    driver.get(i)\n",
    "    ##scrapping job description in next web page(job descriptin page)\n",
    "    try:\n",
    "        job_desc1=driver.find_elements_by_xpath(\"//div[@class='dang-inner-html']\")\n",
    "        job_desc2=driver.find_elements_by_xpath(\"//div[@class='clearboth description']\")\n",
    "        for i in job_desc1: \n",
    "            job_desc.append(i.text.replace(\"\\n\",\" \"))\n",
    "        for i in job_desc2:\n",
    "            job_desc.append(i.text.replace(\"\\n\",\" \"))\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "##create a dataframe of the scraped data.\n",
    "data_A=pd.DataFrame({})\n",
    "data_A['Job_title']=job_title\n",
    "data_A['company_Name']=company\n",
    "data_A['Experience']=exp\n",
    "data_A['Location']=locat\n",
    "data_A['Job desc']=job_desc\n",
    "data_A[0:10]\n",
    "##print(len(job_title),len(job_desc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d4b09a",
   "metadata": {},
   "source": [
    "Q3.\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company_name, experience_required.\n",
    "The location filter to be used is “Delhi/NCR”\n",
    "The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "6b0716d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##opening the webpage\n",
    "url1=\"https://www.naukri.com/\"\n",
    "driver.get(url1)\n",
    "##searching for data scientist\n",
    "designation=driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "designation.send_keys(\"Data Scientist\")\n",
    "##clicking the search button\n",
    "button1=driver.find_element_by_class_name(\"btn\")\n",
    "button1.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "aa4331bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##selecting location filter as delhi\n",
    "loc_filter=driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[3]/div[2]/div[3]/label/i\")\n",
    "loc_filter.click()\n",
    "##selecting salary filter as 3-6 lakhs\n",
    "salary_filter=driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[4]/div[2]/div[2]/label/i\")\n",
    "salary_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "8ae726e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_title</th>\n",
       "      <th>company_Name</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>CBRE South Asia Pvt Ltd</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Immediate Openings For DATA Scientist with 6 T...</td>\n",
       "      <td>Entune IT Consulting Private Limited</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "      <td>Kolkata, Hyderabad/Secunderabad, Pune, Ahmedab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist / Sr. Data Scientist</td>\n",
       "      <td>WEGARNER SOLUTIONS PRIVATE LIMITED</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>Noida, Pune, Mumbai (All Areas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Only Fresher / Data Scientist / Data Analyst /...</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "      <td>Noida, Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hiring Data Scientist Develope || IDS Infotech...</td>\n",
       "      <td>IDS Infotech Ltd.</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Chandigarh, Hyderabad/Secunderabad, Chennai, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hiring Data Scientist Develope || IDS Infotech...</td>\n",
       "      <td>IDS Infotech Ltd.</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Chandigarh, Hyderabad/Secunderabad, Chennai, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist / Data Analyst</td>\n",
       "      <td>CARS24</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Decimal Technologies Pvt Ltd.</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Scientist - Noida</td>\n",
       "      <td>Optum Global Solutions (India) Private Limited</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>R Systems International Ltd.</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Noida(Sector-59 Noida)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_title  \\\n",
       "0                                     Data Scientist   \n",
       "1  Immediate Openings For DATA Scientist with 6 T...   \n",
       "2                Data Scientist / Sr. Data Scientist   \n",
       "3  Only Fresher / Data Scientist / Data Analyst /...   \n",
       "4  Hiring Data Scientist Develope || IDS Infotech...   \n",
       "5  Hiring Data Scientist Develope || IDS Infotech...   \n",
       "6                      Data Scientist / Data Analyst   \n",
       "7                                     Data Scientist   \n",
       "8                      Senior Data Scientist - Noida   \n",
       "9                                     Data Scientist   \n",
       "\n",
       "                                     company_Name Experience  \\\n",
       "0                         CBRE South Asia Pvt Ltd    2-4 Yrs   \n",
       "1            Entune IT Consulting Private Limited    5-8 Yrs   \n",
       "2              WEGARNER SOLUTIONS PRIVATE LIMITED    0-5 Yrs   \n",
       "3                       GABA Consultancy services    0-0 Yrs   \n",
       "4                               IDS Infotech Ltd.   5-10 Yrs   \n",
       "5                               IDS Infotech Ltd.   5-10 Yrs   \n",
       "6                                          CARS24    1-5 Yrs   \n",
       "7                   Decimal Technologies Pvt Ltd.    1-3 Yrs   \n",
       "8  Optum Global Solutions (India) Private Limited    2-6 Yrs   \n",
       "9                    R Systems International Ltd.   5-10 Yrs   \n",
       "\n",
       "                                            Location  \n",
       "0                                   Gurgaon/Gurugram  \n",
       "1  Kolkata, Hyderabad/Secunderabad, Pune, Ahmedab...  \n",
       "2                    Noida, Pune, Mumbai (All Areas)  \n",
       "3               Noida, Gurgaon/Gurugram, Delhi / NCR  \n",
       "4  Chandigarh, Hyderabad/Secunderabad, Chennai, B...  \n",
       "5  Chandigarh, Hyderabad/Secunderabad, Chennai, B...  \n",
       "6                                   Gurgaon/Gurugram  \n",
       "7                                   Gurgaon/Gurugram  \n",
       "8                                              Noida  \n",
       "9                             Noida(Sector-59 Noida)  "
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title=[]\n",
    "company=[]\n",
    "exp=[]\n",
    "locat=[]\n",
    "##scraping web elements job title,name,location,experience\n",
    "job=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "company_name=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "exp_tag=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span[1]\")\n",
    "jobloc=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span[1]\")\n",
    "##extracting text from web elements\n",
    "for i in job: \n",
    "        job_title.append(i.text.replace(\"\\n\",' ')) \n",
    "for i in company_name: \n",
    "        company.append(i.text.replace(\"\\n\",' ')) \n",
    "for i in exp_tag: \n",
    "    exp.append(i.text.replace(\"\\n\",' '))\n",
    "\n",
    "for i in jobloc: \n",
    "    locat.append(i.text.replace(\"\\n\",' '))\n",
    "##creating data frame and printiong first 10 data\n",
    "data_A=pd.DataFrame({})\n",
    "data_A['Job_title']=job_title\n",
    "data_A['company_Name']=company\n",
    "data_A['Experience']=exp\n",
    "data_A['Location']=locat\n",
    "data_A[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c7801",
   "metadata": {},
   "source": [
    "program4\n",
    "Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c1d7b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "url1=\"https://www.glassdoor.co.in/member/home/index.htmn/\"\n",
    "driver.get(url1)\n",
    "##opening the web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "981b5515",
   "metadata": {},
   "outputs": [],
   "source": [
    "##searching as Data Scientist designation in Noida location\n",
    "designation=driver.find_element_by_id(\"sc.keyword\")\n",
    "designation.clear()\n",
    "designation.send_keys(\"Data Scientist\")\n",
    "com=driver.find_element_by_xpath(\"/html/body/header/nav[1]/div/div/div/div[4]/div[2]/form/div/div[2]/div/div[2]/div/ul/li[2]/span\")\n",
    "com\n",
    "loc=driver.find_element_by_id(\"sc.location\")\n",
    "loc.clear()\n",
    "loc.send_keys(\"Noida(India)\")\n",
    "##clicking search button\n",
    "button_s=driver.find_element_by_xpath(\"/html/body/header/nav[1]/div/div/div/div[4]/div[2]/form/div/button\")\n",
    "button_s.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b6fd1c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>no of days ago posted</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uncodemy</td>\n",
       "      <td>1d</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dunnhumby</td>\n",
       "      <td>24h</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Trained Education</td>\n",
       "      <td>24h</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEC Opportunities</td>\n",
       "      <td>2d</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>7d</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ericsson</td>\n",
       "      <td>5d</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Liberin Technologies Private Limited</td>\n",
       "      <td>10d</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Emerging India Analytics</td>\n",
       "      <td>1d</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Crowe</td>\n",
       "      <td>30d+</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pixel Vision</td>\n",
       "      <td>20d</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Company no of days ago posted Rating\n",
       "0                              Uncodemy                    1d       \n",
       "1                             dunnhumby                   24h    4.1\n",
       "2                Data Trained Education                   24h       \n",
       "3                     NEC Opportunities                    2d       \n",
       "4                                                          7d    4.2\n",
       "5                              Ericsson                    5d    4.1\n",
       "6  Liberin Technologies Private Limited                   10d       \n",
       "7              Emerging India Analytics                    1d       \n",
       "8                                 Crowe                  30d+    3.8\n",
       "9                          Pixel Vision                   20d       "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_name=[]\n",
    "date=[]\n",
    "rating=[]\n",
    "##scraping webelement company_name, No. of days ago when job was posted, Rating of the company.\n",
    "date_list=driver.find_elements_by_xpath(\"//div[@class='d-flex align-items-end pl-std css-mi55ob']\")\n",
    "rating_list=driver.find_elements_by_xpath(\"//div[@class='d-flex flex-column css-x75kgh e1rrn5ka3']\")\n",
    "company_list=driver.find_elements_by_xpath(\"//div[@class='d-flex justify-content-between align-items-start']//a//span[1]\")\n",
    "##scraping text of the above webelement\n",
    "for i in company_list: \n",
    "        company_name.append(i.text.strip())\n",
    "for i in date_list: \n",
    "        date.append(i.text.strip())\n",
    "for i in rating_list: \n",
    "        rating.append(i.text.strip())\n",
    "##creating data frame and printing first 10 data\n",
    "data_A=pd.DataFrame({})\n",
    "data_A['Company']=company_name\n",
    "data_A['no of days ago posted']=date\n",
    "data_A['Rating']=rating\n",
    "data_A[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abb3192",
   "metadata": {},
   "source": [
    "program 5\n",
    "Write a python program to scrape the salary data for Data Scientist designation in Noida location.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1d337339",
   "metadata": {},
   "outputs": [],
   "source": [
    "url1=\"https://www.glassdoor.co.in/Salaries/index.htm\"\n",
    "driver.get(url1)\n",
    "\n",
    "##opening the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b895c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "##searching as Data Scientist designation in Noida location\n",
    "designation=driver.find_element_by_id(\"KeywordSearch\")\n",
    "designation.clear()\n",
    "designation.send_keys(\"Data Scientist\")\n",
    "loc=driver.find_element_by_id(\"LocationSearch\")\n",
    "loc.clear()\n",
    "loc.send_keys(\"Noida\")\n",
    "##clicking search button\n",
    "button1=driver.find_element_by_id(\"HeroSearchButton\")\n",
    "button1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "1141d2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20 20\n",
      "['₹3L', '₹6L', '₹6L', '₹5L', '₹4L', '₹8L', '₹5L', '₹6L', '₹8L', '₹4L', '₹2L', '₹6L', '₹10L', '₹4L', '₹8L', '₹9L', '₹8L', '₹12T', '₹4L', '₹25T']\n"
     ]
    }
   ],
   "source": [
    "companylist=[]\n",
    "salarylist=[]\n",
    "rangelist=[]\n",
    "rangelist1=[]\n",
    "ratinglist=[]\n",
    "##scraping webelements Company name, Number of salaries, Average salary, Min salary, Max Salary\n",
    "company=driver.find_elements_by_xpath(\"//a[@class='css-f3vw95 e1aj7ssy3']\")\n",
    "salary=driver.find_elements_by_xpath(\"//div[@class='col-12 col-lg-4 px-lg-0 d-flex align-items-baseline']//h3\")\n",
    "range_max=driver.find_elements_by_xpath(\"//div[@class='d-flex mt-xxsm css-79elbk epuxyqn0']//p[2]\")\n",
    "range_min=driver.find_elements_by_xpath(\"//div[@class='d-flex mt-xxsm css-79elbk epuxyqn0']//p[1]\")\n",
    "rating=driver.find_elements_by_xpath(\"//div[@class='col-12 col-lg-auto']//span[1]\")\n",
    "##scraping text of the above webelements\n",
    "for i in company:\n",
    "    companylist.append(i.text.replace(\"\\n\",' ')) \n",
    "for i in salary:\n",
    "    salarylist.append(i.text.replace(\"\\n\",' ')) \n",
    "for i in range_min: \n",
    "    rangelist.append(i.text.replace(\"\\n\",\"\"))\n",
    "for i in range_max: \n",
    "    rangelist1.append(i.text.replace(\"\\n\",\"\"))\n",
    "for i in rating: \n",
    "    ratinglist.append(i.text.replace(\"\\n\",\"\"))\n",
    "##checking for length of each list\n",
    "\n",
    "print(len(companylist),len(salarylist),len(rangelist),len(rangelist1),len(ratinglist))\n",
    "print(rangelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "7ed762ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_Name</th>\n",
       "      <th>SALARY</th>\n",
       "      <th>MIN_SALARY</th>\n",
       "      <th>Max_SALARY</th>\n",
       "      <th>NO.of salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tata Consultancy Services</td>\n",
       "      <td>₹6,12,205</td>\n",
       "      <td>₹3L</td>\n",
       "      <td>₹13L</td>\n",
       "      <td>18 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IBM</td>\n",
       "      <td>₹9,00,000</td>\n",
       "      <td>₹6L</td>\n",
       "      <td>₹27L</td>\n",
       "      <td>18 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>₹11,63,336</td>\n",
       "      <td>₹6L</td>\n",
       "      <td>₹22L</td>\n",
       "      <td>15 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delhivery</td>\n",
       "      <td>₹12,18,244</td>\n",
       "      <td>₹5L</td>\n",
       "      <td>₹1Cr</td>\n",
       "      <td>15 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ericsson-Worldwide</td>\n",
       "      <td>₹7,39,238</td>\n",
       "      <td>₹4L</td>\n",
       "      <td>₹16L</td>\n",
       "      <td>14 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>₹12,80,000</td>\n",
       "      <td>₹8L</td>\n",
       "      <td>₹15L</td>\n",
       "      <td>14 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Valiance Solutions</td>\n",
       "      <td>₹8,63,750</td>\n",
       "      <td>₹5L</td>\n",
       "      <td>₹15L</td>\n",
       "      <td>10 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EXL Service</td>\n",
       "      <td>₹11,10,000</td>\n",
       "      <td>₹6L</td>\n",
       "      <td>₹15L</td>\n",
       "      <td>9 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Optum</td>\n",
       "      <td>₹14,23,677</td>\n",
       "      <td>₹8L</td>\n",
       "      <td>₹20L</td>\n",
       "      <td>9 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Optum Global Solutions</td>\n",
       "      <td>₹13,28,697</td>\n",
       "      <td>₹4L</td>\n",
       "      <td>₹22L</td>\n",
       "      <td>9 salaries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                company_Name      SALARY MIN_SALARY Max_SALARY NO.of salary\n",
       "0  Tata Consultancy Services   ₹6,12,205        ₹3L       ₹13L  18 salaries\n",
       "1                        IBM   ₹9,00,000        ₹6L       ₹27L  18 salaries\n",
       "2                  Accenture  ₹11,63,336        ₹6L       ₹22L  15 salaries\n",
       "3                  Delhivery  ₹12,18,244        ₹5L       ₹1Cr  15 salaries\n",
       "4         Ericsson-Worldwide   ₹7,39,238        ₹4L       ₹16L  14 salaries\n",
       "5         UnitedHealth Group  ₹12,80,000        ₹8L       ₹15L  14 salaries\n",
       "6         Valiance Solutions   ₹8,63,750        ₹5L       ₹15L  10 salaries\n",
       "7                EXL Service  ₹11,10,000        ₹6L       ₹15L   9 salaries\n",
       "8                      Optum  ₹14,23,677        ₹8L       ₹20L   9 salaries\n",
       "9     Optum Global Solutions  ₹13,28,697        ₹4L       ₹22L   9 salaries"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##creating data frame and printing first 10 data\n",
    "data_A=pd.DataFrame({})\n",
    "data_A['company_Name']=companylist\n",
    "data_A['SALARY']=salarylist\n",
    "data_A['MIN_SALARY']=rangelist\n",
    "data_A['Max_SALARY']=rangelist1\n",
    "data_A['NO.of salary']=ratinglist\n",
    "data_A[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a316ff40",
   "metadata": {},
   "source": [
    "program 6\n",
    "Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5a79bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Go to flipkart webpage by url https://www.flipkart.com/\n",
    "url1=\"https://www.flipkart.com\"\n",
    "driver.get(url1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "4dbed8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Enter “sunglasses” in the search field\n",
    "product=driver.find_element_by_xpath(\"//input[@class='_3704LK']\")\n",
    "product.clear()\n",
    "product.send_keys(\"sunglasses\")\n",
    "##click the search icon\n",
    "search=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5ebf0248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAMIW COLLECTION</td>\n",
       "      <td>UV Protection, Mirrored, Riding Glasses Retro ...</td>\n",
       "      <td>₹199</td>\n",
       "      <td>86% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LIZA ANGEL</td>\n",
       "      <td>Polarized Round Sunglasses (53)</td>\n",
       "      <td>₹279</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer, Sports, Shield, Rectan...</td>\n",
       "      <td>₹599</td>\n",
       "      <td>25% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹699</td>\n",
       "      <td>22% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹499</td>\n",
       "      <td>37% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>UV Protection Oval Sunglasses (56)</td>\n",
       "      <td>₹799</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Round Sunglasses (52)</td>\n",
       "      <td>₹939</td>\n",
       "      <td>27% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>UV Protection Round Sunglasses (Free Size)</td>\n",
       "      <td>₹349</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>PHENOMENAL</td>\n",
       "      <td>UV Protection Round Sunglasses (53)</td>\n",
       "      <td>₹282</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (58)</td>\n",
       "      <td>₹474</td>\n",
       "      <td>68% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               BRAND                                        DESCRIPTION PRICE  \\\n",
       "0   HAMIW COLLECTION  UV Protection, Mirrored, Riding Glasses Retro ...  ₹199   \n",
       "1         LIZA ANGEL                    Polarized Round Sunglasses (53)  ₹279   \n",
       "2           Fastrack  UV Protection Wayfarer, Sports, Shield, Rectan...  ₹599   \n",
       "3           Fastrack   UV Protection Rectangular Sunglasses (Free Size)  ₹699   \n",
       "4           Fastrack      UV Protection Wayfarer Sunglasses (Free Size)  ₹499   \n",
       "..               ...                                                ...   ...   \n",
       "95     VINCENT CHASE                 UV Protection Oval Sunglasses (56)  ₹799   \n",
       "96          Fastrack                UV Protection Round Sunglasses (52)  ₹939   \n",
       "97    kingsunglasses         UV Protection Round Sunglasses (Free Size)  ₹349   \n",
       "98        PHENOMENAL                UV Protection Round Sunglasses (53)  ₹282   \n",
       "99         ROYAL SON          UV Protection Rectangular Sunglasses (58)  ₹474   \n",
       "\n",
       "   DISCOUNT  \n",
       "0   86% off  \n",
       "1   65% off  \n",
       "2   25% off  \n",
       "3   22% off  \n",
       "4   37% off  \n",
       "..      ...  \n",
       "95  60% off  \n",
       "96  27% off  \n",
       "97  78% off  \n",
       "98  87% off  \n",
       "99  68% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##to scrape data from multiple page \n",
    "## get webelement first page link\n",
    "link1=driver.find_elements_by_xpath(\"//a[@class='ge-49M _2Kfbh8']\")\n",
    "link=driver.find_elements_by_xpath(\"//a[@class='ge-49M']\")\n",
    "url_link=[]\n",
    "brand=[]\n",
    "desc=[]\n",
    "price=[]\n",
    "discount=[]\n",
    "##get webelements of all page link\n",
    "for i in link1:\n",
    "    url_link.append(i.get_attribute(\"href\"))\n",
    "for i in link:\n",
    "    url_link.append(i.get_attribute(\"href\"))\n",
    "##get webelements of first 3 page link\n",
    "for i in url_link[0:3]:\n",
    "    driver.get(i)\n",
    "    ##scrape web elements of brand,price,description and discount from multiple page\n",
    "    try:\n",
    "        brand_list=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "        desc_list1=driver.find_elements_by_xpath(\"//a[@class='IRpwTa _2-ICcC']\")\n",
    "        desc_list=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "        price_list=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "        discount_list=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']//span[1]\")##scraping text from web elements[brand,price,description and discount]\n",
    "    ##scraping text from web elements[brand,price,description and discount]\n",
    "        for i in brand_list:\n",
    "            brand.append(i.text.strip())\n",
    "        for i in desc_list1:\n",
    "            desc.append(i.text.replace(\"/n\",\"\"))\n",
    "        for i in desc_list:\n",
    "            desc.append(i.text.replace(\"/n\",\"\"))\n",
    "        for i in price_list:\n",
    "            price.append(i.text.strip())\n",
    "        for i in discount_list: \n",
    "            discount.append(i.text.strip())\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "##creating data frame and printing first 100 data\n",
    "    data=pd.DataFrame({})\n",
    "    data['BRAND']=brand\n",
    "    data['DESCRIPTION']=desc\n",
    "    data['PRICE']=price\n",
    "    data['DISCOUNT']=discount\n",
    "data[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b5942",
   "metadata": {},
   "source": [
    "progrsam 7\n",
    "Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace.\n",
    "1. Rating\n",
    "2. Review_summary\n",
    "3. Full review\n",
    "You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "5bb016bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RATING</th>\n",
       "      <th>REVIEW</th>\n",
       "      <th>REVIEW SUMMARY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific purchase</td>\n",
       "      <td>Awesome product Worth of Money👌👌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Just okay</td>\n",
       "      <td>The product we received is not what we ordered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Really Nice</td>\n",
       "      <td>Nice product.... in this price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Very nice. Comfortable for him .tknq soo much 😊😊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>Lightweight good looking awesome shoeFinishing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>3</td>\n",
       "      <td>Just okay</td>\n",
       "      <td>The product we received is not what we ordered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4</td>\n",
       "      <td>Really Nice</td>\n",
       "      <td>Nice product.... in this price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Very nice. Comfortable for him .tknq soo much 😊😊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>Useless product</td>\n",
       "      <td>Poor quality,And look at the toe end, Both the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>Useless product</td>\n",
       "      <td>Poor quality,And look at the toe end, Both the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RATING             REVIEW  \\\n",
       "0       5  Terrific purchase   \n",
       "1       3          Just okay   \n",
       "2       4        Really Nice   \n",
       "3       5     Simply awesome   \n",
       "4       4          Very Good   \n",
       "..    ...                ...   \n",
       "95      3          Just okay   \n",
       "96      4        Really Nice   \n",
       "97      5     Simply awesome   \n",
       "98      1    Useless product   \n",
       "99      1    Useless product   \n",
       "\n",
       "                                       REVIEW SUMMARY  \n",
       "0                    Awesome product Worth of Money👌👌  \n",
       "1   The product we received is not what we ordered...  \n",
       "2                      Nice product.... in this price  \n",
       "3    Very nice. Comfortable for him .tknq soo much 😊😊  \n",
       "4   Lightweight good looking awesome shoeFinishing...  \n",
       "..                                                ...  \n",
       "95  The product we received is not what we ordered...  \n",
       "96                     Nice product.... in this price  \n",
       "97   Very nice. Comfortable for him .tknq soo much 😊😊  \n",
       "98  Poor quality,And look at the toe end, Both the...  \n",
       "99  Poor quality,And look at the toe end, Both the...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rating=[]\n",
    "review=[]\n",
    "review_s=[]\n",
    "url_link=[]\n",
    "##go to first web page\n",
    "link1=driver.find_elements_by_xpath(\"//a[@class='ge-49M _2Kfbh8']\")\n",
    "##getting webelemengt of  next page\n",
    "link=driver.find_elements_by_xpath(\"//a[@class='ge-49M']\")\n",
    "rating_list=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "review_list=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "for i in link1:\n",
    "    url_link.append(i.get_attribute(\"href\"))\n",
    "for i in link:\n",
    "    url_link.append(i.get_attribute(\"href\"))\n",
    "##GO to next page\n",
    "for i in url_link[0:10]:\n",
    "    driver.get(i)\n",
    "##Scraping 1. Rating 2. Review_summary 3. Full review for first 100 reviews\n",
    "\n",
    "    try:\n",
    "        rating_list=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "        rating_list1=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1rdVr6 _1BLPMq']\")\n",
    "        review_list=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "        summary_list=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "        for i in rating_list:\n",
    "            rating.append(i.text.replace(\"\\n\",\"\"))\n",
    "        for i in rating_list1:\n",
    "            rating.append(i.text.replace(\"\\n\",\"\"))\n",
    "        for i in review_list:\n",
    "            review.append(i.text.replace(\"\\n\",\"\"))\n",
    "        for i in summary_list:\n",
    "            review_s.append(i.text.replace(\"\\n\",\"\"))\n",
    "    except:\n",
    "        pass\n",
    "##creating data frame and printing first 100 reviews\n",
    "data=pd.DataFrame({})\n",
    "data['RATING']=rating\n",
    "data['REVIEW']=review\n",
    "data['REVIEW SUMMARY']=review_s\n",
    "##print(len(rating),len(review),len(review_s))\n",
    "data       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab3b31",
   "metadata": {},
   "source": [
    "program 8\n",
    "Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "7cef0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "##go to flipkart.com\n",
    "url1=\"https://www.flipkart.com\"\n",
    "driver.get(url1)\n",
    "##search for “sneakers” in the search field\n",
    "product=driver.find_element_by_xpath(\"//input[@class='_3704LK']\")\n",
    "##to clear search field\n",
    "product.clear()\n",
    "product.send_keys(\"sneakers\")\n",
    "##click search button\n",
    "search=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "2f41f667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>Shoes For Boys Dance shoes Sneakers For Men</td>\n",
       "      <td>₹2,319</td>\n",
       "      <td>53% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>9019 Sneakers For Men</td>\n",
       "      <td>₹1,478</td>\n",
       "      <td>57% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SPARX</td>\n",
       "      <td>Puma Rebound LayUp SL Sneakers For Men</td>\n",
       "      <td>₹753</td>\n",
       "      <td>24% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DUCATI</td>\n",
       "      <td>Flex Renew SlipOn Sneakers For Men</td>\n",
       "      <td>₹1,289</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Axter</td>\n",
       "      <td>SM-322 Sneakers For Men</td>\n",
       "      <td>₹299</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹399</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>FILA</td>\n",
       "      <td>Modern Trendy Sneakers Shoes Sneakers For Men</td>\n",
       "      <td>₹845</td>\n",
       "      <td>68% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>SPARX</td>\n",
       "      <td>Fashionable casual sneakers shoes Sneakers For...</td>\n",
       "      <td>₹629</td>\n",
       "      <td>16% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Ktiz</td>\n",
       "      <td>Combo Pack Of 4 Casual Shoes Loafer Shoes Snea...</td>\n",
       "      <td>₹384</td>\n",
       "      <td>61% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Strollin</td>\n",
       "      <td>MESTO Sneakers For Men</td>\n",
       "      <td>₹399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       BRAND                                        DESCRIPTION   PRICE  \\\n",
       "0       PUMA        Shoes For Boys Dance shoes Sneakers For Men  ₹2,319   \n",
       "1       PUMA                              9019 Sneakers For Men  ₹1,478   \n",
       "2      SPARX             Puma Rebound LayUp SL Sneakers For Men    ₹753   \n",
       "3     DUCATI                 Flex Renew SlipOn Sneakers For Men  ₹1,289   \n",
       "4      Axter                            SM-322 Sneakers For Men    ₹299   \n",
       "..       ...                                                ...     ...   \n",
       "95    BRUTON                                   Sneakers For Men    ₹399   \n",
       "96      FILA      Modern Trendy Sneakers Shoes Sneakers For Men    ₹845   \n",
       "97     SPARX  Fashionable casual sneakers shoes Sneakers For...    ₹629   \n",
       "98      Ktiz  Combo Pack Of 4 Casual Shoes Loafer Shoes Snea...    ₹384   \n",
       "99  Strollin                             MESTO Sneakers For Men    ₹399   \n",
       "\n",
       "   DISCOUNT  \n",
       "0   53% off  \n",
       "1   57% off  \n",
       "2   24% off  \n",
       "3   65% off  \n",
       "4   78% off  \n",
       "..      ...  \n",
       "95  88% off  \n",
       "96  68% off  \n",
       "97  16% off  \n",
       "98  61% off  \n",
       "99  60% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##go to first web page\n",
    "link1=driver.find_elements_by_xpath(\"//a[@class='ge-49M _2Kfbh8']\")\n",
    "##getting webelemengt of  next page\n",
    "link=driver.find_elements_by_xpath(\"//a[@class='ge-49M']\")\n",
    "url_link=[]\n",
    "brand=[]\n",
    "desc=[]\n",
    "price=[]\n",
    "discount=[]\n",
    "for i in link1:\n",
    "    url_link.append(i.get_attribute(\"href\"))\n",
    "for i in link:\n",
    "    url_link.append(i.get_attribute(\"href\"))\n",
    "##GO to next page untill it reaches 100 data\n",
    "for i in url_link[0:4]:\n",
    "    driver.get(i)\n",
    "##scrape web elements of brand,price,description and discount from multiple page\n",
    "\n",
    "    try:\n",
    "        brand_list=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "        desc_list1=driver.find_elements_by_xpath(\"//a[@class='IRpwTa _2-ICcC']\")\n",
    "        desc_list=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "        price_list=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "        discount_list=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']//span[1]\")\n",
    "        for i in brand_list:\n",
    "            brand.append(i.text.strip())\n",
    "        for i in desc_list1:\n",
    "            desc.append(i.text.replace(\"/n\",\"\"))\n",
    "        for i in desc_list:\n",
    "            desc.append(i.text.replace(\"/n\",\"\"))\n",
    "        for i in price_list:\n",
    "            price.append(i.text.strip())\n",
    "        for i in discount_list: \n",
    "            discount.append(i.text.strip())\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "##creating data frame and printing first 100 reviews\n",
    "    data=pd.DataFrame({})\n",
    "    data['BRAND']=brand\n",
    "    data['DESCRIPTION']=desc\n",
    "    data['PRICE']=price\n",
    "    data['DISCOUNT']=discount\n",
    "data[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf568137",
   "metadata": {},
   "source": [
    "program 9\n",
    "Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”, as shown in the below image\n",
    "scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "a35d54b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Newfeel By Decathlon</td>\n",
       "      <td>Men Walking Shoes</td>\n",
       "      <td>Rs. 899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MENGLER</td>\n",
       "      <td>Men Walking Shoes</td>\n",
       "      <td>Rs. 645Rs. 1699(62% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Unisex Skateboarding Shoes</td>\n",
       "      <td>Rs. 4995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Skechers</td>\n",
       "      <td>Women GO WALK 5 Walking Shoes</td>\n",
       "      <td>Rs. 3363Rs. 5799(42% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Red Tape</td>\n",
       "      <td>Men Walking Shoes</td>\n",
       "      <td>Rs. 1474Rs. 5899(75% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>DEAS</td>\n",
       "      <td>Women Woven Design Sneakers</td>\n",
       "      <td>Rs. 649Rs. 999(35% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Bata</td>\n",
       "      <td>Men Solid Leather Formal Derbys</td>\n",
       "      <td>Rs. 1699Rs. 1999(15% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ASIAN</td>\n",
       "      <td>Men Running Shoes</td>\n",
       "      <td>Rs. 799Rs. 999(Rs. 200 OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Solid Sneakers</td>\n",
       "      <td>Rs. 10999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>H&amp;M</td>\n",
       "      <td>Men Slip-On Trainers</td>\n",
       "      <td>Rs. 1049Rs. 1499(30% OFF)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   BRAND                      DESCRIPTION  \\\n",
       "0   Newfeel By Decathlon                Men Walking Shoes   \n",
       "1                MENGLER                Men Walking Shoes   \n",
       "2                   Nike       Unisex Skateboarding Shoes   \n",
       "3               Skechers    Women GO WALK 5 Walking Shoes   \n",
       "4               Red Tape                Men Walking Shoes   \n",
       "..                   ...                              ...   \n",
       "95                  DEAS      Women Woven Design Sneakers   \n",
       "96                  Bata  Men Solid Leather Formal Derbys   \n",
       "97                 ASIAN                Men Running Shoes   \n",
       "98                  Geox               Men Solid Sneakers   \n",
       "99                   H&M             Men Slip-On Trainers   \n",
       "\n",
       "                          PRICE  \n",
       "0                       Rs. 899  \n",
       "1      Rs. 645Rs. 1699(62% OFF)  \n",
       "2                      Rs. 4995  \n",
       "3     Rs. 3363Rs. 5799(42% OFF)  \n",
       "4     Rs. 1474Rs. 5899(75% OFF)  \n",
       "..                          ...  \n",
       "95      Rs. 649Rs. 999(35% OFF)  \n",
       "96    Rs. 1699Rs. 1999(15% OFF)  \n",
       "97  Rs. 799Rs. 999(Rs. 200 OFF)  \n",
       "98                    Rs. 10999  \n",
       "99    Rs. 1049Rs. 1499(30% OFF)  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Go to the link - https://www.myntra.com/shoes \n",
    "url=\"https://www.myntra.com/shoes\"\n",
    "driver.get(url)\n",
    "##Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”\n",
    "price_filter=driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label/div')\n",
    "price_filter.click()\n",
    "color_filter=driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label/div')\n",
    "color_filter.click()\n",
    "link=driver.find_elements_by_xpath(\"//li[@class='pagination-number']//a[1]\")\n",
    "url_link=[]\n",
    "brand=[]\n",
    "desc=[]\n",
    "price=[]\n",
    "url_link.append(url)\n",
    "for i in link:\n",
    "    url_link.append(i.get_attribute(\"href\"))\n",
    "url_link\n",
    "##GO to next page untill it reaches 100 shoes\n",
    "for i in url_link[0:2]:\n",
    "    driver.get(i)\n",
    "##scraping web elements of “Brand” of the shoes , Short Shoe description, price for 100 shoes\n",
    "    try:\n",
    "        brand_list=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "        desc_list=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "        ##desc_list=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "        price_list=driver.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "##scraping text of web elements “Brand” of the shoes , Short Shoe description, price for 100 shoes\n",
    "        for i in brand_list:\n",
    "            brand.append(i.text.strip())\n",
    "        for i in desc_list:\n",
    "            desc.append(i.text.replace(\"/n\",\"\"))\n",
    "        for i in price_list:\n",
    "            price.append(i.text.strip())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    data=pd.DataFrame({})\n",
    "    data['BRAND']=brand\n",
    "    data['DESCRIPTION']=desc\n",
    "    data['PRICE']=price\n",
    "data[0:100]\n",
    "##print(len(brand),len(desc),len(price))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f7131",
   "metadata": {},
   "source": [
    "Q10: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i5”\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    " titleRatings Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "58139941",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Go to webpage https://www.amazon.in\n",
    "url=\" https://www.amazon.in/\"\n",
    "driver.get(url)\n",
    "##Enter “Laptop” in the search field\n",
    "product=driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "product.clear()\n",
    "product.send_keys(\"laptop\")\n",
    "##click the search icon\n",
    "search=driver.find_element_by_id(\"nav-search-submit-button\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "98fe95ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set CPU Type filter to “Intel Core i5”\n",
    "cpu_filter=driver.find_element_by_xpath(\"//li[@id='p_n_feature_thirteen_browse-bin/12598163031']//span[1]\")\n",
    "cpu_filter.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "02fb9e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "##set CPU Type filter to “Intel Core i7”\n",
    "cpu_filter1=driver.find_element_by_xpath(\"//li[@id='p_n_feature_thirteen_browse-bin/12598162031']//span[1]\")\n",
    "cpu_filter1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "42981e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HP 15 (2021) Thin &amp; Light 11th Gen Core i5 Lap...</td>\n",
       "      <td>61,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acer Aspire 3 core i5 11th Generation Processo...</td>\n",
       "      <td>55,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Renewed) Fujitsu Intel Core i5 3340M 15.6-Inc...</td>\n",
       "      <td>21,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HP Pavilion Gaming 10th Gen Intel Core i5 Proc...</td>\n",
       "      <td>63,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lenovo IdeaPad Slim 5 11th Gen Intel Core i5 1...</td>\n",
       "      <td>61,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dell Inspiron 3501 15.6\" FHD Display Laptop (i...</td>\n",
       "      <td>63,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HP Pavilion (2021) 11th Gen Core i5 Laptop, 16...</td>\n",
       "      <td>73,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HP Pavilion (2021) Thin &amp; Light 11th Gen Core ...</td>\n",
       "      <td>84,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lenovo IdeaPad Gaming 3 10th Gen Intel Core i5...</td>\n",
       "      <td>57,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HP 15 (2021) Thin &amp; Light 11th Gen Core i5 Lap...</td>\n",
       "      <td>61,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE   PRICE\n",
       "0  HP 15 (2021) Thin & Light 11th Gen Core i5 Lap...  61,990\n",
       "1  Acer Aspire 3 core i5 11th Generation Processo...  55,600\n",
       "2  (Renewed) Fujitsu Intel Core i5 3340M 15.6-Inc...  21,990\n",
       "3  HP Pavilion Gaming 10th Gen Intel Core i5 Proc...  63,990\n",
       "4  Lenovo IdeaPad Slim 5 11th Gen Intel Core i5 1...  61,990\n",
       "5  Dell Inspiron 3501 15.6\" FHD Display Laptop (i...  63,300\n",
       "6  HP Pavilion (2021) 11th Gen Core i5 Laptop, 16...  73,990\n",
       "7  HP Pavilion (2021) Thin & Light 11th Gen Core ...  84,990\n",
       "8  Lenovo IdeaPad Gaming 3 10th Gen Intel Core i5...  57,990\n",
       "9  HP 15 (2021) Thin & Light 11th Gen Core i5 Lap...  61,990"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title=[]\n",
    "price=[]\n",
    "rating=[]\n",
    "##scraping first 10 laptops data.which inclue:title, Ratings,Price\n",
    "price_list=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "rating_list=driver.find_elements_by_xpath(\"//span[@class='a-size-base']\")\n",
    "title_list=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "##scraping the text for above webelement\n",
    "for i in title_list:\n",
    "    title.append(i.text.strip())\n",
    "for i in price_list:\n",
    "    price.append(i.text.strip())\n",
    "for i in rating_list:\n",
    "    rating.append(i.text.strip())\n",
    "##create dataframe for scraped data and print it\n",
    "data=pd.DataFrame({})\n",
    "data['TITLE']=title\n",
    "data['PRICE']=price\n",
    "data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed9a684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13a09b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
